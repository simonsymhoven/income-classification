{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Income Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Daten laden und Dataframe erzeugen](#data)\n",
    "2. [Deskriptive Statistik](#descriptive)\n",
    "3. [Visualisierung und Daten aufbereiten](#visual)\n",
    "4. [Kodierung & Skalierung der Daten](#scale)\n",
    "5. [PCA](#pca)\n",
    "6. [Clustering](#clustering)\n",
    "7. [Modelle](#models)\n",
    "    * [Logit mit original Daten](#logit)\n",
    "    * [Logit mit PCA](#logit-pca)\n",
    "    * [Logit mit nicht lineaer transformierten Daten](#logit-transformed)\n",
    "    * [Random Forest](#random-forest)\n",
    "    * [Neuronales Netz](#neruonal)\n",
    "8. [Vergleich der Modelle](#compare)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhaltliche Fragen:\n",
    "- Logit Modell mit transformierten Daten abchecken\n",
    "- PCA?!\n",
    "- Cluster Analyse absegnen lassen\n",
    "\n",
    "## Offene Todos\n",
    "- Vergleich der Modelle\n",
    "- Neuronales Netz mit PCA Daten\n",
    "- neuronales Netz speichern und laden\n",
    "- Random Forest mit PCA Daten\n",
    "- Sensitivität und  von confusion matrix\n",
    "- Notebook dokumentieren\n",
    "- cpaital-gain und capital loss deckeln statt logartihmieren "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import tensorflow as tf\n",
    "from matplotlib.patches import Patch\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import statsmodels\n",
    "from statsmodels.discrete.discrete_model import Logit \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "from sklearn.cluster import KMeans\n",
    "import keras_tuner\n",
    "import keras\n",
    "import matplotlib.ticker as mticker\n",
    "import math\n",
    "from sklearn.tree import plot_tree"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten laden und Dataframe erzeugen <a class=\"data\" id=\"data\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('adult.data', delimiter=',', skipinitialspace=True, na_values=['?'])\n",
    "test_df = pd.read_csv('adult.test', delimiter=',', skipinitialspace=True, na_values=['?'])\n",
    "df = pd.concat([train_df, test_df])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df.index = range(0, len(df))\n",
    "df_size = df.shape[0]\n",
    "\n",
    "print(f'Dataset Size: {df_size}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir verändern die Zielgröße und verwenden **1** für die Klasse **>50k** und **0** für die Klasse **<=50k**. Es fäält auf, dass **education-num** bereits die Variable **education** als numerische Werte abbildet, alleridngs von 0 bis 16. Das ist unbrauchbar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['income'] = df['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
    "df = df.drop('education-num', axis= 1)\n",
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Datensatz enthält einige **NaN** Werte. Diese ersetzen wir durch die jeweils am häufigsten vorkommenden Werte der jeweiligen Input-Variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['workclass'].fillna(df['workclass'].mode()[0], inplace=True)\n",
    "df['occupation'].fillna(df['occupation'].mode()[0], inplace=True)\n",
    "df['native-country'].fillna(df['native-country'].mode()[0], inplace=True)   \n",
    "\n",
    "df['workclass'].fillna(df['workclass'].mode()[0], inplace=True)\n",
    "df['occupation'].fillna(df['occupation'].mode()[0], inplace=True)\n",
    "df['native-country'].fillna(df['native-country'].mode()[0], inplace=True)\n",
    "\n",
    "df.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deskriptive Statistik <a class=\"descriptive\" id=\"descriptive\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerische Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric = [var for var in df.columns if df[var].dtype!=object]\n",
    "numeric.remove(\"income\")\n",
    "print('\\nThere are {} numeric variables:\\n\\n{}'.format(len(numeric), numeric))\n",
    "\n",
    "df.describe().T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kategoriale Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = [var for var in df.columns if df[var].dtype==object]\n",
    "\n",
    "\n",
    "print('There are {} categorical variables:\\n'.format(len(categorical)))\n",
    "\n",
    "for var in categorical:\n",
    "\n",
    "    print(f'{var} contains {len(df[var].unique())} labels')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisierung und Daten aufbereiten <a class=\"visual\" id=\"visual\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir die Daten weiter bearbeiten, schauen wir uns zunächst einmal verschiedene Visualisierungen dazu an, um entscheiden zu können, was wir mit den Daten machen:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verteilung von **income**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "df['income'].value_counts().plot.pie(labels=[r'$\\leq50$k', r'$>50$k'], autopct='%1.2f%%', wedgeprops = {\"linewidth\": 1, \"edgecolor\": \"white\"})\n",
    "plt.title('Income Distribution')\n",
    "plt.tight_layout()\n",
    "plt.ylabel('')\n",
    "plt.savefig('figs/income_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Korrelationsmatrix der numerischen Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8), dpi=100)\n",
    "sns.heatmap(df[numeric].corr(), cmap=\"viridis\", fmt=\".2%\",linewidth=0.5, annot=True, vmin=-1, vmax=1)\n",
    "formatter = mticker.FuncFormatter(lambda x, _: f\"{x*100:.0f}%\")\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.yaxis.set_major_formatter(formatter)\n",
    "plt.title(\"Correlation of numeric variables\")\n",
    "plt.savefig('figs/correlation.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerische Variablen bezogen auf **income**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df.drop(categorical, axis=1), hue='income', corner=True)\n",
    "plt.savefig('figs/pair_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histogramm und Boxplots der numerischen Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=len(numeric), figsize=(20, 8))\n",
    "fig.suptitle(\"Histogram & Boxplot of numeric variables\")\n",
    "\n",
    "for i, variable in enumerate(numeric):\n",
    "    sns.histplot(data=df, x=variable, hue='income', ax=axes[0][i], kde=True, stat='density')\n",
    "    sns.boxplot(data=df, x=variable, y='income', ax=axes[1][i], orient='h')\n",
    "\n",
    "    axes[0][i].set_xlabel(None)\n",
    "    axes[0][i].set_ylabel(None)\n",
    "    axes[0][i].set_title(variable)\n",
    "    axes[0][i].legend(title='Income', labels=['$>50$k', '$\\leq50$k'])\n",
    "\n",
    "    axes[1][i].set_xlabel(None)\n",
    "    axes[1][i].set_ylabel(None)\n",
    "    axes[1][i].set_title(variable)\n",
    "    axes[1][i].set_yticklabels(['$\\leq50$k', '$>50$k'])\n",
    "\n",
    "    median = df[variable].median()\n",
    "    axes[1][i].axvline(median, linestyle='--', color='black')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/histogram_boxplot.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausreißer behandeln"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir transformieren **capital-gain** und **capital-loss** mit dem log(x+1) um Ausreißer zu behandeln. x+1, da die features Werte gleich 0 enthalten und dafür der log nicht definiert ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['capital-gain'] = np.log(df['capital-gain']+1)\n",
    "df['capital-loss'] = np.log(df['capital-loss']+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=len(numeric), figsize=(20, 8))\n",
    "fig.suptitle(\"Histogram & Boxplot of numeric variables\")\n",
    "\n",
    "\n",
    "for i, variable in enumerate(numeric):\n",
    "    sns.histplot(data=df, x=variable, hue='income', ax=axes[0][i], kde=True, stat='density')\n",
    "    sns.boxplot(data=df, x=variable, y='income', ax=axes[1][i], orient='h')\n",
    "\n",
    "    axes[0][i].set_xlabel(None)\n",
    "    axes[0][i].set_ylabel(None)\n",
    "    axes[0][i].set_title(variable)\n",
    "    axes[0][i].legend(title='Income', labels=['$>50$k', '$\\leq50$k'])\n",
    "\n",
    "    axes[1][i].set_xlabel(None)\n",
    "    axes[1][i].set_ylabel(None)\n",
    "    axes[1][i].set_title(variable)\n",
    "    axes[1][i].set_yticklabels(['$\\leq50$k', '$>50$k'])\n",
    "\n",
    "    median = df[variable].median()\n",
    "    axes[1][i].axvline(median, linestyle='--', color='black')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/histogram_boxplot_modified.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verteilung der kategorialen Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=2\n",
    "cols=4\n",
    "\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(5*cols, 6*rows), sharey=True)\n",
    "fig.supylabel('Percentage')\n",
    "\n",
    "for i, variable in enumerate(categorical):\n",
    "    row = i // cols\n",
    "    col = i % cols\n",
    "    ax = axes[row][col]\n",
    "    pct_counts = df[variable].value_counts(normalize=True)\n",
    "    sns.barplot(x=pct_counts.index, y=pct_counts.values, ax=ax)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "    yticks = mtick.PercentFormatter(1)\n",
    "    ax.yaxis.set_major_formatter(yticks)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_title('Distribution of {}'.format(variable))\n",
    "\n",
    "    for j, p in enumerate(ax.patches):\n",
    "        ax.text(p.get_x() + p.get_width() / 2, p.get_height() + 0.05, '{:.0%}'.format(pct_counts.values[j]), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/distribution_categorical.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verteilung der kategorialen Variablen bezogen auf **income**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=2\n",
    "cols=4\n",
    "\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(5*cols, 6*rows), sharey=True)\n",
    "fig.suptitle('Distribution of variables in relation to income')\n",
    "fig.supxlabel('Income')\n",
    "fig.supylabel('Percentage')\n",
    "\n",
    "for i, variable in enumerate(categorical):\n",
    "    row = i // cols\n",
    "    col = i % cols\n",
    "    ax = axes[row][col]\n",
    "    \n",
    "    ct = pd.crosstab(df['income'], df[variable])\n",
    "    ct_pct = ct.apply(lambda r: r/r.sum(), axis=1)\n",
    "\n",
    "    ct_pct.plot(kind='bar', stacked=True, ax=ax)\n",
    "    \n",
    "    ax.set_xticklabels([r'$\\leq50$k', r'$>50$k'], rotation=0)\n",
    "    yticks = mtick.PercentFormatter(1)\n",
    "    ax.yaxis.set_major_formatter(yticks)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    sorted_labels = sorted(labels, key=lambda x: -ct[x].sum())\n",
    "    top_labels = sorted_labels[:5]\n",
    "    remaining_labels = sorted_labels[5:]\n",
    "    top_handles = [handles[labels.index(l)] for l in top_labels]\n",
    "    remaining_sum = sum([ct[x].sum() for x in remaining_labels])\n",
    "    top_labels.append(f'Other ({remaining_sum})')\n",
    "    top_handles.append(Patch(facecolor='gray', edgecolor='black'))\n",
    "    ax.legend(top_handles, top_labels, loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=2)\n",
    "\n",
    "\n",
    "    for i in ax.containers:\n",
    "        ax.bar_label(i, label_type='center', color=\"white\", fontsize=10, fmt='{:.0%}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/distribution_categroical_hue_income.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun schauen wir, wie die Klassen **0** und **1** innerhalb der kategorialen Variablen verteilt sind:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_counts = df.groupby('race')['income'].value_counts().unstack()\n",
    "print(race_counts)\n",
    "\n",
    "df = df.drop('race',axis= 1)\n",
    "categorical.remove('race')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupation_counts = df.groupby('occupation')['income'].value_counts().unstack()\n",
    "print(occupation_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_labels = [\"Adm-clerical\", \"Armed-Forces\", \"Craft-repair\", \"Farming-fishing\", \"Handlers-cleaners\", \"Machine-op-inspct\", \"Priv-house-serv\", \"Protective-serv\", \"Transport-moving\"]\n",
    "new_label = \"blue\" # handwerk\n",
    "\n",
    "df.loc[df[\"occupation\"].isin(old_labels), \"occupation\"] = new_label\n",
    "\n",
    "old_labels = [\"Exec-managerial\", \"Other-service\", \"Prof-specialty\", \"Sales\", \"Tech-support\"]\n",
    "new_label = \"white\" # büro\n",
    "\n",
    "df.loc[df[\"occupation\"].isin(old_labels), \"occupation\"] = new_label\n",
    "occupation_counts = df.groupby('occupation')['income'].value_counts().unstack()\n",
    "print(occupation_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_counts = df.groupby('education')['income'].value_counts().unstack().fillna(0)\n",
    "print(education_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_labels = [\"Preschool\", \"1st-4th\", \"5th-6th\", \"7th-8th\", \"9th\", \"10th\", \"11th\", \"12th\"]\n",
    "new_label = \"Pre-Highschool\"\n",
    "\n",
    "df.loc[df[\"education\"].isin(old_labels), \"education\"] = new_label\n",
    "\n",
    "old_labels = [\"Bachelors\", \"Masters\", \"Doctorate\", \"Assoc-acdm\", \"Assoc-voc\", \"Prof-school\", \"Some-college\"]\n",
    "new_label = \"Academic\"\n",
    "\n",
    "df.loc[df[\"education\"].isin(old_labels), \"education\"] = new_label\n",
    "\n",
    "education_counts = df.groupby('education')['income'].value_counts().unstack().fillna(0)\n",
    "print(education_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workclass_counts = df.groupby('workclass')['income'].value_counts().unstack().fillna(0)\n",
    "print(workclass_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df[df['workclass'] == 'Never-worked'].index)\n",
    "df = df.drop(df[df['workclass'] == 'Without-pay'].index)\n",
    "\n",
    "\n",
    "old_labels = [\"Federal-gov\", \"Local-gov\", \"State-gov\"]\n",
    "new_label = \"Gov\"\n",
    "\n",
    "df.loc[df[\"workclass\"].isin(old_labels), \"workclass\"] = new_label\n",
    "\n",
    "workclass_counts = df.groupby('workclass')['income'].value_counts().unstack().fillna(0)\n",
    "print(workclass_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_counts = df.groupby('relationship')['income'].value_counts().unstack().fillna(0)\n",
    "print(relationship_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_labels = [\"Husband\", \"Wife\"]\n",
    "new_label = \"Married\"\n",
    "\n",
    "df.loc[df[\"relationship\"].isin(old_labels), \"relationship\"] = new_label\n",
    "\n",
    "relationship_counts = df.groupby('relationship')['income'].value_counts().unstack().fillna(0)\n",
    "print(relationship_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_counts = df.groupby('sex')['income'].value_counts().unstack().fillna(0)\n",
    "print(sex_counts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Martial Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "marital_counts = df.groupby('marital-status')['income'].value_counts().unstack().fillna(0)\n",
    "print(marital_counts)\n",
    "\n",
    "df = df.drop('marital-status', axis= 1)\n",
    "categorical.remove('marital-status')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_country_counts = df.groupby('native-country')['income'].value_counts().unstack().fillna(0)\n",
    "print(native_country_counts)\n",
    "\n",
    "df = df.drop('native-country',axis= 1)\n",
    "categorical.remove('native-country')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=2\n",
    "cols=3\n",
    "\n",
    "fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(5*cols, 6*rows), sharey=True)\n",
    "fig.suptitle('Distribution of variables in relation to income')\n",
    "fig.supxlabel('Income')\n",
    "fig.supylabel('Percentage')\n",
    "\n",
    "for i, variable in enumerate(categorical):\n",
    "    row = i // cols\n",
    "    col = i % cols\n",
    "    ax = axes[row][col]\n",
    "    \n",
    "    ct = pd.crosstab(df['income'], df[variable])\n",
    "    ct_pct = ct.apply(lambda r: r/r.sum(), axis=1)\n",
    "\n",
    "    ct_pct.plot(kind='bar', stacked=True, ax=ax)\n",
    "    \n",
    "    ax.set_xticklabels([r'$\\leq50$k', r'$>50$k'], rotation=0)\n",
    "    yticks = mtick.PercentFormatter(1)\n",
    "    ax.yaxis.set_major_formatter(yticks)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlabel('')\n",
    "\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    sorted_labels = sorted(labels, key=lambda x: -ct[x].sum())\n",
    "    top_labels = sorted_labels[:5]\n",
    "    remaining_labels = sorted_labels[5:]\n",
    "    top_handles = [handles[labels.index(l)] for l in top_labels]\n",
    "    remaining_sum = sum([ct[x].sum() for x in remaining_labels])\n",
    "    top_labels.append(f'Other ({remaining_sum})')\n",
    "    top_handles.append(Patch(facecolor='gray', edgecolor='black'))\n",
    "    ax.legend(top_handles, top_labels, loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=2)\n",
    "\n",
    "\n",
    "    for i in ax.containers:\n",
    "        ax.bar_label(i, label_type='center', color=\"white\", fontsize=10, fmt='{:.0%}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/distribution_categroical_hue_income_modified.png')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kodierung & Skalierung der Daten <a class=\"scale\" id=\"scale\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OneHot Codierung der kategorialen Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataframe Shape before encoding: {df.shape}')\n",
    "\n",
    "onehot_encoder = ce.OneHotEncoder(cols=categorical)\n",
    "df = onehot_encoder.fit_transform(df)\n",
    "\n",
    "df = df.drop([\"workclass_4\", \"education_3\", \"occupation_2\", \"relationship_5\", \"sex_2\"], axis=1)\n",
    "\n",
    "print(f'Dataframe Shape after encoding: {df.shape}')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skalierung der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finale Datensätze"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datensatz 1 (aufbereitet und one-hot codiert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.drop(\"income\", axis=1), df[\"income\"], test_size=0.2, shuffle=False)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datensatz 2 (aufbereitet, one-hot codiert & min-max skaliert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled, x_test_scaled, y_train_scaled, y_test_scaled = train_test_split(df_scaled.drop(\"income\", axis=1), df_scaled[\"income\"], test_size=0.2, shuffle=False)\n",
    "df_scaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA <a class=\"pca\" id=\"pca\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = PCA(n_components=2)\n",
    "pca_2_comp = pca_2.fit_transform(x_train_scaled)\n",
    "\n",
    "df_pca_2 = pd.DataFrame(data=pca_2_comp, columns=['principal component 1', 'principal component 2'])\n",
    "df_pca_2['income'] = y_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nDescription')\n",
    "print(df_pca_2.describe())\n",
    "\n",
    "print('\\nInfo')\n",
    "print(df_pca_2.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(df_pca_2['principal component 1'],df_pca_2['principal component 2'],c=df_pca_2['income'],cmap='prism', s =5)\n",
    "plt.xlabel('pc1')\n",
    "plt.ylabel('pc2')\n",
    "plt.savefig('figs/pca.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "df_pca = pca.fit_transform(x_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "plt.scatter(PC_values, pca.explained_variance_ratio_)\n",
    "\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Hauptkomponenten')\n",
    "plt.ylabel('Erklärte Varianz')\n",
    "plt.savefig('figs/scree_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Hauptkomponenten')\n",
    "plt.ylabel('Varianz')\n",
    "plt.title('Kumuluative erklärte Varianz')\n",
    "plt.scatter(PC_values, np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.savefig('figs/variance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_8 = PCA(n_components=8)\n",
    "df_pca_8 = pca_8.fit_transform(x_train_scaled)\n",
    "df_pca_8_test = pca_8.transform(x_test_scaled)\n",
    "\n",
    "# Konstante 1er Spalte hinzufügen für Beta0\n",
    "df_pca_8 = statsmodels.tools.add_constant(df_pca_8)\n",
    "df_pca_8_test = statsmodels.tools.add_constant(df_pca_8_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering <a class=\"clustering\" id=\"clustering\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_combinations = list(combinations(numeric, 2))\n",
    "\n",
    "for combination in numeric_combinations:\n",
    "    first = combination[0]\n",
    "    second = combination[1]\n",
    "    x = df[first]\n",
    "    y = df[second]\n",
    "    data = pd.concat([x, y], axis=1)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=2, random_state=None, n_init='auto')\n",
    "    kmeans.fit(data)\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.scatterplot(data=df, x=first, y=second, hue=kmeans.labels_)\n",
    "    plt.xlabel(first)\n",
    "    plt.ylabel(second)\n",
    "    plt.title('K-means cluster analysis')\n",
    "\n",
    "    plt.subplot(1, 2, 2, sharey=plt.gca())\n",
    "    sns.scatterplot(data=df, x=first, y=second, hue='income')\n",
    "    plt.xlabel(first)\n",
    "    plt.ylabel('')\n",
    "    plt.title('Real Cluster')\n",
    "\n",
    "    plt.suptitle(f\"{first} vs {second}\", y=0.95, fontsize=12, fontweight='bold') \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.savefig(f'figs/{first}-{second}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelle <a class=\"model\" id=\"model\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konstante 1er Spale für Beta0 hinzufügen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = statsmodels.tools.add_constant(x_train)\n",
    "x_test = statsmodels.tools.add_constant(x_test)\n",
    "x_train.head()\n",
    "\n",
    "x_train_scaled = statsmodels.tools.add_constant(x_train_scaled)\n",
    "x_test_scaled = statsmodels.tools.add_constant(x_test_scaled)\n",
    "x_train_scaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit <a class=\"logit\" id=\"logit\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Logit(y_train_scaled, x_train_scaled).fit(method='newton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.mle_retvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary2())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = list(map(round, y_predict))\n",
    "  \n",
    "print('Actual values', list(y_test_scaled.values))\n",
    "print('Predictions :', prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_scaled, prediction) \n",
    "cmn = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "accuracy = accuracy_score(y_test_scaled, prediction)\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=100)\n",
    "sns.heatmap(cmn, cmap=\"viridis\", linewidth=0.5, fmt=\".2%\", annot=True, vmin=0, vmax=1)\n",
    "formatter = mticker.FuncFormatter(lambda x, _: f\"{x*100:.0f}%\")\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.yaxis.set_major_formatter(formatter)\n",
    "plt.title(f\"Logit - scaled ({accuracy:.2%})\")\n",
    "plt.savefig('figs/logit-scaled.png')\n",
    "plt.show()\n",
    "\n",
    "print(f'Test accuracy = {accuracy_score(y_test_scaled, prediction).round(4)*100} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit mit PCA <a class=\"logit-pca\" id=\"logit-pca\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca = Logit(y_train_scaled, df_pca_8).fit(method='newton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pca.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_scaled = model_pca.predict(df_pca_8_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction2 = list(map(round, y_predict_scaled))\n",
    "  \n",
    "# comparing original and predicted values of y\n",
    "print('Actual values', list(y_test_scaled.values))\n",
    "print('Predictions :', prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm2 = confusion_matrix(y_test_scaled, prediction2) \n",
    "cmn2 = cm2.astype('float') / cm2.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "accuracy = accuracy_score(y_test_scaled, prediction2)\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=100)\n",
    "sns.heatmap(cmn2, cmap=\"viridis\", linewidth=0.5, fmt=\".2%\", annot=True, vmin=0, vmax=1)\n",
    "formatter = mticker.FuncFormatter(lambda x, _: f\"{x*100:.0f}%\")\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.yaxis.set_major_formatter(formatter)\n",
    "plt.title(f\"Logit - PCA ({accuracy:.2%})\")\n",
    "plt.savefig('figs/logit-pca.png')\n",
    "plt.show()\n",
    "  \n",
    "# accuracy score of the model\n",
    "print(f'Test accuracy = {accuracy_score(y_test_scaled, prediction2).round(4)*100} %')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logit mit nicht lineaer transformierten Daten <a class=\"logit-transformed\" id=\"logit-transformed\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['age'] = np.log(x_train['age'])\n",
    "x_train['fnlwgt'] = np.log(x_train['fnlwgt'])\n",
    "\n",
    "x_test['age'] = np.log(x_test['age'])\n",
    "x_test['fnlwgt'] = np.log(x_test['fnlwgt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformed = Logit(y_train, x_train).fit(method='newton')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformed.summary2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_transformed = model_transformed.predict(x_test)\n",
    "prediction_transformed = list(map(round, y_predict_transformed))\n",
    "  \n",
    "print('Actual values', list(y_test.values))\n",
    "print('Predictions :', prediction_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm3 = confusion_matrix(y_test, prediction_transformed) \n",
    "cmn3 = cm3.astype('float') / cm3.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "accuracy = accuracy_score(y_test, prediction_transformed)\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=100)\n",
    "sns.heatmap(cmn3, cmap=\"viridis\", linewidth=0.5, fmt=\".2%\", annot=True, vmin=0, vmax=1)\n",
    "formatter = mticker.FuncFormatter(lambda x, _: f\"{x*100:.0f}%\")\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.yaxis.set_major_formatter(formatter)\n",
    "plt.title(f\"Logit - log transformed ({accuracy:.2%})\")\n",
    "plt.savefig('figs/logit-log-transformed.png')\n",
    "plt.show()\n",
    "  \n",
    "print(f'Test accuracy = {accuracy:.2%} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_pca_8.drop(\"const\", axis=1)\n",
    "x_train.drop(\"const\", axis=1)\n",
    "x_train_scaled.drop(\"const\", axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest <a class=\"random\" id=\"random\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "y_predict = forest.predict(x_test_scaled)\n",
    " \n",
    "forest_score_accuracy = forest.score(x_test_scaled, y_test_scaled)\n",
    "\n",
    "cm_forest = confusion_matrix(y_test_scaled, y_predict)\n",
    "cm_forest_normalized = cm_forest.astype('float') / cm_forest.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=100)\n",
    "\n",
    "sns.heatmap(cm_forest_normalized, cmap='viridis', annot=True, fmt=\".2%\", vmin=0, vmax=1)\n",
    "plt.title(f'Random Forest ({forest_score_accuracy:.2%})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "formatter = mticker.FuncFormatter(lambda x, _: f\"{x*100:.0f}%\")\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/random-forest.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "plot_tree(forest.estimators_[0], \n",
    "    feature_names=x_train_scaled.columns,\n",
    "    class_names=['$\\leq50$k', '$>50$k'], \n",
    "    filled=True, impurity=True, \n",
    "    rounded=True)\n",
    "fig.savefig('random_forest_model.png')\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [100, 110, 120, 130, 140],\n",
    "    'max_features': [2, 3, 4, 5],\n",
    "    'min_samples_leaf': [5, 6, 7, 8],\n",
    "    'min_samples_split': [12, 14, 16, 18],\n",
    "    'n_estimators': [30, 40, 45, 50, 60]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_tuned = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=forest_tuned, param_grid=param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(x_train_scaled, y_train_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest_tuned_new = RandomForestClassifier(**grid_search.best_params_)\n",
    "\n",
    "forest_tuned_new.fit(x_train_scaled, y_train_scaled)\n",
    "\n",
    "y_predict = forest_tuned_new.predict(x_test_scaled)\n",
    " \n",
    "forest_score_accuracy = forest_tuned_new.score(x_test_scaled, y_test_scaled)\n",
    "\n",
    "cm_forest = confusion_matrix(y_test_scaled, y_predict)\n",
    "cm_forest_normalized = cm_forest.astype('float') / cm_forest.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=100)\n",
    "\n",
    "sns.heatmap(cm_forest_normalized, annot=True, fmt=\".2%\", cmap='viridis', vmin=0, vmax=1)\n",
    "plt.title(f'Random Forest - tuned ({forest_score_accuracy:.2%})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "formatter = mticker.FuncFormatter(lambda x, _: f\"{x*100:.0f}%\")\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/random-tuned.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuronales Netz <a class=\"neruonal\" id=\"neruonal\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 250\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(units=21, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=2, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = model.fit(x_train_scaled, y_train_scaled, validation_split=0.1, batch_size=16, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, to_file='nn_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].plot(history.history['val_accuracy'])\n",
    "axs[0].set_title('Model Accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].plot(history.history['val_loss'])\n",
    "axs[1].set_title('Model Loss')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/nn_accuracy_loss.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict(x_test_scaled)\n",
    "y_pred = np.round(y_pred_prob).flatten()\n",
    "\n",
    "cm_nn = confusion_matrix(y_test_scaled, y_pred)\n",
    "cm_nn_normalized = cm_nn.astype('float') / cm_nn.sum(axis=1)[:, np.newaxis]\n",
    "nn_accuracy = accuracy_score(y_test_scaled, y_pred)\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=100)\n",
    "\n",
    "sns.heatmap(cm_nn_normalized, annot=True, fmt=\".2%\", cmap='viridis', vmin=0, vmax=1)\n",
    "plt.title(f'Neuronal Network ({nn_accuracy:.2%})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "formatter = mticker.FuncFormatter(lambda x, _: f\"{x*100:.0f}%\")\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/nn_confusion_matrix.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(\n",
    "        keras.layers.Dense(\n",
    "            units=hp.Int(\"layer1\", min_value=15, max_value=21, step=1),\n",
    "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "        )    \n",
    "    ),\n",
    "    model.add(\n",
    "        keras.layers.Dense(\n",
    "            units=hp.Int(\"layer2\", min_value=10, max_value=15, step=1),\n",
    "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "        )    \n",
    "    ),\n",
    "    model.add(\n",
    "        keras.layers.Dense(\n",
    "            units=hp.Int(\"layer3\", min_value=5, max_value=10, step=1),\n",
    "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "        )\n",
    "    ),\n",
    "    model.add(\n",
    "        keras.layers.Dense(\n",
    "            units=hp.Int(\"layer4\", min_value=2, max_value=5, step=1),\n",
    "            activation=hp.Choice(\"activation\", [\"relu\", \"tanh\"]),\n",
    "        )\n",
    "    ),\n",
    "    model.add(keras.layers.Dense(1, activation=\"sigmoid\")),\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate), \n",
    "        loss=\"binary_crossentropy\", \n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = keras_tuner.Hyperband(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_epochs=epochs,\n",
    "    overwrite=True,\n",
    "    factor=3,\n",
    "    directory='neuronal-network',\n",
    "    project_name='income-classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()\n",
    "X_train_scaled, X_val_scaled, Y_train_scaled, Y_val_scaled = train_test_split(x_train_scaled, y_train_scaled, shuffle=False, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20)\n",
    "\n",
    "tuner.search(X_train_scaled, Y_train_scaled, epochs=epochs, validation_data=(X_val_scaled, Y_val_scaled), \n",
    "             callbacks=[stop_early])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.values)\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "layer is {best_hps.get('layer1')} and the optimal learning rate for the optimizer\n",
    "is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n",
    "\n",
    "best_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = best_model.fit(X_train_scaled, Y_train_scaled, validation_data=(X_val_scaled, Y_val_scaled), epochs=epochs, verbose=0)\n",
    "\n",
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(best_model, to_file='nn_best_model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].plot(history.history['val_accuracy'])\n",
    "axs[0].set_title('Model Accuracy')\n",
    "axs[0].set_ylabel('Accuracy')\n",
    "axs[0].set_xlabel('Epoch')\n",
    "axs[0].legend(['Train', 'Validation'], loc='upper left')\n",
    "\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].plot(history.history['val_loss'])\n",
    "axs[1].set_title('Model Loss')\n",
    "axs[1].set_ylabel('Loss')\n",
    "axs[1].set_xlabel('Epoch')\n",
    "axs[1].legend(['Train', 'Validation'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/nn_accuracy_loss_tuned.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob = best_model.predict(x_test_scaled)\n",
    "y_pred = np.round(y_pred_prob).flatten()\n",
    "\n",
    "cm_nn = confusion_matrix(y_test_scaled, y_pred)\n",
    "cm_nn_normalized = cm_nn.astype('float') / cm_nn.sum(axis=1)[:, np.newaxis]\n",
    "nn_accuracy = accuracy_score(y_test_scaled, y_pred)\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=100)\n",
    "\n",
    "sns.heatmap(cm_nn_normalized, annot=True, fmt=\".2%\", cmap='viridis', vmin=0, vmax=1)\n",
    "plt.title(f'Neuronal Network - tuned ({nn_accuracy:.2%})')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "\n",
    "formatter = mticker.FuncFormatter(lambda x, _: f\"{x*100:.0f}%\")\n",
    "cax = plt.gcf().axes[-1]\n",
    "cax.yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('figs/nn_tuned_confusion_matrix.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vergleich der Modelle <a class=\"compare\" id=\"compare\"></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
